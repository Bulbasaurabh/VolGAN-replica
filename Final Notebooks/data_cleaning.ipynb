{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION: All Settings in One Place ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm, skew as scipy_skew\n",
    "from scipy.interpolate import griddata, interp1d\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# IV Surface Grid\n",
    "M_GRID = np.linspace(0.6, 1.4, 10)              # Moneyness grid (10 points)\n",
    "DTE_GRID = [7, 14, 30, 60, 91, 122, 152, 182]   # Days to expiration (8 points)\n",
    "\n",
    "# Data Files\n",
    "OPTIONS_PARQUET = \"../data/options_dataset.parquet\"\n",
    "OM_UNDERLYING_CSV = \"../data/OMunderlying.csv\"\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 1: Data Loading Functions ===\n",
    "\n",
    "def load_om_data_for_volgan(options_path, underlying_path):\n",
    "    \"\"\"Load and prepare OptionMetrics data for VolGAN.\"\"\"\n",
    "    # Load options\n",
    "    df_opt = pd.read_parquet(options_path)\n",
    "    df_opt = df_opt[df_opt['ticker'] == 'SPX'].copy()\n",
    "    df_opt['date'] = pd.to_datetime(df_opt['date'])\n",
    "    df_opt['exdate'] = pd.to_datetime(df_opt['exdate'])\n",
    "    \n",
    "    # Strike scaling\n",
    "    if df_opt['strike_price'].median() > 1e4:\n",
    "        df_opt['strike_price'] = df_opt['strike_price'] / 1000.0\n",
    "    \n",
    "    df_opt['dte'] = (df_opt['exdate'] - df_opt['date']).dt.days\n",
    "    df_opt['mid'] = (df_opt['best_bid'] + df_opt['best_offer']) / 2.0\n",
    "    \n",
    "    # Load spot\n",
    "    df_spot = pd.read_csv(underlying_path)\n",
    "    df_spot['date'] = pd.to_datetime(df_spot['date'])\n",
    "    df_spot = df_spot[df_spot['ticker'] == 'SPX'][['date', 'close']].rename(columns={'close': 'spot'})\n",
    "    df_spot = df_spot.drop_duplicates('date').sort_values('date')\n",
    "    \n",
    "    # Merge\n",
    "    df_opt = df_opt.merge(df_spot, on='date', how='inner')\n",
    "    df_opt['moneyness'] = df_opt['strike_price'] / df_opt['spot']\n",
    "    \n",
    "    return df_opt, df_spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 2: Load and Quality-Filter Data ===\n",
    "\n",
    "df_options, df_underlying = load_om_data_for_volgan(OPTIONS_PARQUET, OM_UNDERLYING_CSV)\n",
    "print(f\"Raw data loaded: {len(df_options)} option rows\")\n",
    "\n",
    "# Quality Screening\n",
    "df = df_options.copy()\n",
    "df['moneyness'] = df['strike_price'] / df['spot']\n",
    "\n",
    "# Quality filters\n",
    "df_1m = df_1m.replace([np.inf, -np.inf], np.nan)\n",
    "df_1m = df_1m.dropna(subset=['mid', 'delta', 'vega', 'best_bid', 'best_offer'])\n",
    "df_1m = df_1m[df_1m['best_offer'] >= df_1m['best_bid']].copy()\n",
    "\n",
    "# Compute spread metrics\n",
    "df_1m['spread'] = (df_1m['best_offer'] - df_1m['best_bid']).clip(lower=0)\n",
    "df_1m['rel_spread'] = (df_1m['spread'] / df_1m['mid']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Liquidity screens\n",
    "liq_mask = (\n",
    "    (df_1m['mid'] > 0.05) &\n",
    "    (df_1m['rel_spread'] <= 0.35) &\n",
    "    df_1m['delta'].between(-1.1, 1.1) &\n",
    "    (df_1m['vega'] > 0)\n",
    ")\n",
    "df_1m = df_1m[liq_mask].copy()\n",
    "\n",
    "print(f\"After quality filters: {len(df_1m)} rows\")\n",
    "print(f\"Date range: {df_1m['date'].min().date()} â†’ {df_1m['date'].max().date()}\")\n",
    "print(f\"Coverage: {len(df_1m['date'].unique())} unique trading days\\n\")\n",
    "\n",
    "# Keep relevant columns\n",
    "cols_keep = ['date', 'exdate', 'cp_flag', 'strike_price', 'spot', 'moneyness', \n",
    "             'dte', 'mid', 'best_bid', 'best_offer', 'volume', 'open_interest', \n",
    "             'delta', 'vega', 'impl_volatility', 'spread', 'rel_spread']\n",
    "cols_available = [c for c in cols_keep if c in df_1m.columns]\n",
    "df_1m_filtered = df_1m[cols_available].sort_values(['date', 'dte', 'strike_price']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Final filtered dataset: {len(df_1m_filtered)} rows\\n\")\n",
    "\n",
    "df_1m_filtered.to_csv('FilteredDataset.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 3: Build IV Surface Grid ===\n",
    "\n",
    "def build_iv_surface_grid_robust(df_opt, m_grid, dte_grid):\n",
    "    \"\"\"Build IV surface with robust handling of days with insufficient data.\"\"\"\n",
    "    dates = sorted(df_opt['date'].unique())\n",
    "    surfaces = []\n",
    "    valid_dates = []\n",
    "    \n",
    "    for d in tqdm(dates, desc=\"Building IV surfaces\"):\n",
    "        day_df = df_opt[df_opt['date'] == d]\n",
    "        points = day_df[['moneyness', 'dte', 'impl_volatility']].dropna()\n",
    "        \n",
    "        if len(points) < 10:\n",
    "            continue\n",
    "        \n",
    "        unique_dte = points['dte'].nunique()\n",
    "        unique_moneyness = points['moneyness'].nunique()\n",
    "        \n",
    "        if unique_dte < 2 or unique_moneyness < 3:\n",
    "            continue\n",
    "        \n",
    "        dte_min, dte_max = points['dte'].min(), points['dte'].max()\n",
    "        if dte_max - dte_min < 10:\n",
    "            continue\n",
    "        \n",
    "        grid_m, grid_dte = np.meshgrid(m_grid, dte_grid)\n",
    "        \n",
    "        try:\n",
    "            iv_surface = griddata(\n",
    "                points[['moneyness', 'dte']].values,\n",
    "                points['impl_volatility'].values,\n",
    "                (grid_m, grid_dte),\n",
    "                method='linear',\n",
    "                fill_value=np.nan\n",
    "            )\n",
    "            \n",
    "            if np.isnan(iv_surface).any():\n",
    "                iv_surface_filled = griddata(\n",
    "                    points[['moneyness', 'dte']].values,\n",
    "                    points['impl_volatility'].values,\n",
    "                    (grid_m, grid_dte),\n",
    "                    method='nearest'\n",
    "                )\n",
    "                iv_surface = np.where(np.isnan(iv_surface), iv_surface_filled, iv_surface)\n",
    "            \n",
    "            if np.isnan(iv_surface).sum() > 0.5 * iv_surface.size:\n",
    "                continue\n",
    "            \n",
    "            surfaces.append(iv_surface.T.flatten())\n",
    "            valid_dates.append(d)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if len(surfaces) == 0:\n",
    "        raise ValueError(\"No valid surfaces could be constructed!\")\n",
    "    \n",
    "    surfaces_array = np.array(surfaces)\n",
    "    tau_grid_years = np.array(dte_grid) / 365.0\n",
    "    \n",
    "    print(f\"Surface construction: {len(valid_dates)}/{len(dates)} days ({100*len(valid_dates)/len(dates):.1f}%)\")\n",
    "    \n",
    "    return surfaces_array, valid_dates, m_grid, tau_grid_years\n",
    "\n",
    "surfaces_transform, dates_volgan, m, tau = build_iv_surface_grid_robust(\n",
    "    df_1m_filtered, M_GRID, DTE_GRID\n",
    ")\n",
    "print(f\"Surface grid shape: {surfaces_transform.shape}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
