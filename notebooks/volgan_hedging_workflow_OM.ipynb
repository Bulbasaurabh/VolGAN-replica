{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09576c6d",
   "metadata": {},
   "source": [
    "# VolGAN Hedging — Adapted for OptionMetrics Data\n",
    "\n",
    "This notebook implements VolGAN-based hedging for SPX straddles using OptionMetrics data,\n",
    "making it directly comparable with delta-only and delta-vega strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115c614",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 1: Imports and Configuration ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm\n",
    "from scipy.interpolate import griddata, interp1d\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Configuration\n",
    "TRADING_DAYS_PER_MONTH = 21\n",
    "OPTIONS_PARQUET = \"../data/options_dataset.parquet\"\n",
    "OM_UNDERLYING_CSV = \"../data/OMunderlying.csv\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3bb906",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 2: Data Loading Functions ===\n",
    "\n",
    "def load_om_data_for_volgan(options_path, underlying_path):\n",
    "    \"\"\"Load and prepare OptionMetrics data for VolGAN.\"\"\"\n",
    "    # Load options\n",
    "    df_opt = pd.read_parquet(options_path)\n",
    "    df_opt = df_opt[df_opt['ticker'] == 'SPX'].copy()\n",
    "    df_opt['date'] = pd.to_datetime(df_opt['date'])\n",
    "    df_opt['exdate'] = pd.to_datetime(df_opt['exdate'])\n",
    "    \n",
    "    # Strike scaling\n",
    "    if df_opt['strike_price'].median() > 1e4:\n",
    "        df_opt['strike_price'] = df_opt['strike_price'] / 1000.0\n",
    "    \n",
    "    df_opt['dte'] = (df_opt['exdate'] - df_opt['date']).dt.days\n",
    "    df_opt['mid'] = (df_opt['best_bid'] + df_opt['best_offer']) / 2.0\n",
    "    \n",
    "    # Load spot\n",
    "    df_spot = pd.read_csv(underlying_path)\n",
    "    df_spot['date'] = pd.to_datetime(df_spot['date'])\n",
    "    df_spot = df_spot[df_spot['ticker'] == 'SPX'][['date', 'close']].rename(columns={'close': 'spot'})\n",
    "    df_spot = df_spot.drop_duplicates('date').sort_values('date')\n",
    "    \n",
    "    # Merge\n",
    "    df_opt = df_opt.merge(df_spot, on='date', how='inner')\n",
    "    df_opt['moneyness'] = df_opt['strike_price'] / df_opt['spot']\n",
    "    \n",
    "    return df_opt, df_spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b516066",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 3: Load Raw Data ===\n",
    "\n",
    "df_options, df_underlying = load_om_data_for_volgan(OPTIONS_PARQUET, OM_UNDERLYING_CSV)\n",
    "print(f\"Loaded: {len(df_options)} option rows, {len(df_underlying)} spot rows\")\n",
    "print(f\"Date range: {df_options['date'].min()} to {df_options['date'].max()}\")\n",
    "df_options.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f2eb7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 4: Build IV Surface Grid ===\n",
    "\n",
    "def build_iv_surface_grid(df_opt, m_grid=np.linspace(0.6, 1.4, 10), \n",
    "                          dte_grid=[7, 14, 30, 60, 91, 122, 152, 182]):\n",
    "    \"\"\"Construct implied volatility surfaces on a fixed grid.\"\"\"\n",
    "    dates = sorted(df_opt['date'].unique())\n",
    "    surfaces = []\n",
    "    \n",
    "    for d in tqdm(dates, desc=\"Building IV surfaces\"):\n",
    "        day_df = df_opt[df_opt['date'] == d]\n",
    "        points = day_df[['moneyness', 'dte', 'impl_volatility']].dropna()\n",
    "        \n",
    "        if len(points) < 10:\n",
    "            continue\n",
    "        \n",
    "        grid_m, grid_dte = np.meshgrid(m_grid, dte_grid)\n",
    "        \n",
    "        iv_surface = griddata(\n",
    "            points[['moneyness', 'dte']].values,\n",
    "            points['impl_volatility'].values,\n",
    "            (grid_m, grid_dte),\n",
    "            method='linear',\n",
    "            fill_value=np.nan\n",
    "        )\n",
    "        \n",
    "        # Fill NaNs with nearest neighbor\n",
    "        if np.isnan(iv_surface).any():\n",
    "            iv_surface_filled = griddata(\n",
    "                points[['moneyness', 'dte']].values,\n",
    "                points['impl_volatility'].values,\n",
    "                (grid_m, grid_dte),\n",
    "                method='nearest'\n",
    "            )\n",
    "            iv_surface = np.where(np.isnan(iv_surface), iv_surface_filled, iv_surface)\n",
    "        \n",
    "        surfaces.append(iv_surface.T.flatten())\n",
    "    \n",
    "    surfaces_transform = np.array(surfaces)\n",
    "    tau_grid = np.array(dte_grid) / 365.0\n",
    "    \n",
    "    return surfaces_transform, dates[:len(surfaces)], m_grid, tau_grid\n",
    "\n",
    "m_grid = np.linspace(0.6, 1.4, 10)\n",
    "dte_grid = [7, 14, 30, 60, 91, 122, 152, 182]\n",
    "\n",
    "surfaces_transform, dates_volgan, m, tau = build_iv_surface_grid(df_options, m_grid, dte_grid)\n",
    "print(f\"Surface grid shape: {surfaces_transform.shape} (should be T x 80)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f284d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 5: Prepare VolGAN Training Data ===\n",
    "\n",
    "def prepare_volgan_data_om(surfaces, dates_list, df_spot):\n",
    "    \"\"\"Prepare condition and target arrays for VolGAN.\"\"\"\n",
    "    dates_pd = pd.to_datetime(dates_list)\n",
    "    spot_df = df_spot[df_spot['date'].isin(dates_pd)].sort_values('date')\n",
    "    prices = spot_df['spot'].values\n",
    "    \n",
    "    # Log returns\n",
    "    prices_prev = np.zeros(len(prices))\n",
    "    prices_prev[1:] = prices[:-1]\n",
    "    prices_prev[0] = prices[0]\n",
    "    log_rtn = np.log(prices) - np.log(prices_prev)\n",
    "    \n",
    "    # Realized volatility (21-day rolling)\n",
    "    realised_vol_tm1 = np.zeros(len(log_rtn) - 22)\n",
    "    for i in range(len(realised_vol_tm1)):\n",
    "        realised_vol_tm1[i] = np.sqrt(252 / 21) * np.sqrt(np.sum(log_rtn[i:(i+21)]**2))\n",
    "    \n",
    "    # Align dates\n",
    "    dates_t = dates_pd[22:]\n",
    "    log_rtn_t = log_rtn[22:]\n",
    "    log_rtn_tm1 = np.sqrt(252) * log_rtn[21:-1]\n",
    "    log_rtn_tm2 = np.sqrt(252) * log_rtn[20:-2]\n",
    "    \n",
    "    # Log IV surfaces\n",
    "    log_iv_t = np.log(surfaces[22:])\n",
    "    log_iv_tm1 = np.log(surfaces[21:-1])\n",
    "    log_iv_inc_t = log_iv_t - log_iv_tm1\n",
    "    \n",
    "    # Condition: [r_{t-1}, r_{t-2}, RV_{t-1}, log_iv_{t-1}]\n",
    "    condition = np.concatenate((\n",
    "        np.expand_dims(log_rtn_tm1, axis=1),\n",
    "        np.expand_dims(log_rtn_tm2, axis=1),\n",
    "        np.expand_dims(realised_vol_tm1, axis=1),\n",
    "        log_iv_tm1\n",
    "    ), axis=1)\n",
    "    \n",
    "    # Target: [r_t, log_iv_inc_t]\n",
    "    log_rtn_t_ann = np.sqrt(252) * log_rtn_t\n",
    "    true = np.concatenate((\n",
    "        np.expand_dims(log_rtn_t_ann, axis=1),\n",
    "        log_iv_inc_t\n",
    "    ), axis=1)\n",
    "    \n",
    "    return true, condition, dates_t\n",
    "\n",
    "true, condition, dates_volgan_aligned = prepare_volgan_data_om(\n",
    "    surfaces_transform, dates_volgan, df_underlying\n",
    ")\n",
    "print(f\"VolGAN data shapes: true={true.shape}, condition={condition.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7d0d7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 5B: Train/Test Split for VolGAN Training ===\n",
    "\n",
    "# Define train/test cutoff\n",
    "train_end = pd.Timestamp('2020-12-31')\n",
    "dates_pd = pd.to_datetime(dates_volgan_aligned)\n",
    "\n",
    "# Create masks\n",
    "train_mask = dates_pd <= train_end\n",
    "test_mask = dates_pd > train_end\n",
    "\n",
    "# Convert to tensors and split\n",
    "true_train = torch.tensor(true[train_mask], dtype=torch.float, device=device)\n",
    "true_test = torch.tensor(true[test_mask], dtype=torch.float, device=device)\n",
    "condition_train = torch.tensor(condition[train_mask], dtype=torch.float, device=device)\n",
    "condition_test = torch.tensor(condition[test_mask], dtype=torch.float, device=device)\n",
    "\n",
    "print(f\"Training data split:\")\n",
    "print(f\"  - Train samples: {true_train.shape[0]} (dates up to {train_end.date()})\")\n",
    "print(f\"  - Test samples: {true_test.shape[0]} (dates after {train_end.date()})\")\n",
    "print(f\"  - Condition dim: {condition_train.shape[1]}\")\n",
    "print(f\"  - Target dim: {true_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24218f34",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 6: VolGAN Model Classes ===\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, cond_dim, hidden_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.linear1 = nn.Linear(noise_dim + cond_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.linear3 = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.activation = nn.Softplus()\n",
    "    \n",
    "    def forward(self, noise, condition):\n",
    "        out = torch.cat([noise, condition], dim=-1).to(torch.float)\n",
    "        out = self.activation(self.linear1(out))\n",
    "        out = self.activation(self.linear2(out))\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.linear1(x))\n",
    "        out = self.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be178c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 7: Black-Scholes Pricing Functions ===\n",
    "\n",
    "def BS_call(S, tau, K, sigma, r=0):\n",
    "    d1 = (np.log(S/K) + tau*(r + 0.5*sigma**2)) / (sigma*np.sqrt(tau))\n",
    "    d2 = d1 - sigma*np.sqrt(tau)\n",
    "    return S*norm.cdf(d1) - K*norm.cdf(d2)*np.exp(-r*tau)\n",
    "\n",
    "def BS_put(S, tau, K, sigma, r=0):\n",
    "    d1 = (np.log(S/K) + tau*(r + 0.5*sigma**2)) / (sigma*np.sqrt(tau))\n",
    "    d2 = d1 - sigma*np.sqrt(tau)\n",
    "    return K*norm.cdf(-d2)*np.exp(-r*tau) - S*norm.cdf(-d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9e6fc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 8: VolGAN Hedging Function ===\n",
    "\n",
    "def hedge_with_volgan(gen, condition_t, spot_t, K_straddle, hedge_options, \n",
    "                      m_grid, tau_grid, n_scenarios=500, device='cpu'):\n",
    "    \"\"\"Generate scenarios and compute LASSO hedge ratios.\"\"\"\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn((n_scenarios, gen.noise_dim), device=device)\n",
    "        condition_batch = condition_t.unsqueeze(0).repeat(n_scenarios, 1)\n",
    "        fake = gen(noise, condition_batch)\n",
    "        spot_ret_scenarios = fake[:, 0].cpu().numpy() / np.sqrt(252)\n",
    "        log_iv_inc_scenarios = fake[:, 1:].cpu().numpy()\n",
    "    \n",
    "    spot_t1_scenarios = spot_t * np.exp(spot_ret_scenarios)\n",
    "    iv_current = np.exp(condition_t[3:].cpu().numpy())\n",
    "    \n",
    "    V_scenarios = []\n",
    "    for i in range(n_scenarios):\n",
    "        S_t1 = spot_t1_scenarios[i]\n",
    "        iv_t1 = iv_current * np.exp(log_iv_inc_scenarios[i])\n",
    "        iv_surface = iv_t1.reshape(len(m_grid), len(tau_grid))\n",
    "        \n",
    "        m_straddle = K_straddle / S_t1\n",
    "        tau_straddle = 21 / 365\n",
    "        \n",
    "        iv_at_tau = interp1d(tau_grid, iv_surface, axis=1, \n",
    "                             bounds_error=False, fill_value='extrapolate')(tau_straddle)\n",
    "        iv_interp = np.clip(np.interp(m_straddle, m_grid, iv_at_tau), 0.01, 2.0)\n",
    "        \n",
    "        V_call = BS_call(S_t1, tau_straddle, K_straddle, iv_interp, r=0)\n",
    "        V_put = BS_put(S_t1, tau_straddle, K_straddle, iv_interp, r=0)\n",
    "        V_scenarios.append(V_call + V_put)\n",
    "    \n",
    "    V_scenarios = np.array(V_scenarios)\n",
    "    \n",
    "    # Hedge instruments\n",
    "    H_scenarios = {'spot': spot_t1_scenarios}\n",
    "    \n",
    "    for opt in hedge_options:\n",
    "        H_values = []\n",
    "        for i in range(n_scenarios):\n",
    "            S_t1 = spot_t1_scenarios[i]\n",
    "            iv_t1 = iv_current * np.exp(log_iv_inc_scenarios[i])\n",
    "            iv_surface = iv_t1.reshape(len(m_grid), len(tau_grid))\n",
    "            \n",
    "            m_opt = opt['K'] / S_t1\n",
    "            tau_opt = 21 / 365\n",
    "            \n",
    "            iv_at_tau = interp1d(tau_grid, iv_surface, axis=1, \n",
    "                                 bounds_error=False, fill_value='extrapolate')(tau_opt)\n",
    "            iv_opt = np.clip(np.interp(m_opt, m_grid, iv_at_tau), 0.01, 2.0)\n",
    "            \n",
    "            if opt['cp'] == 'C':\n",
    "                H_val = BS_call(S_t1, tau_opt, opt['K'], iv_opt, r=0)\n",
    "            else:\n",
    "                H_val = BS_put(S_t1, tau_opt, opt['K'], iv_opt, r=0)\n",
    "            H_values.append(H_val)\n",
    "        \n",
    "        H_scenarios[f\"{opt['cp']}_{opt['K']}\"] = np.array(H_values)\n",
    "    \n",
    "    # LASSO regression\n",
    "    X = pd.DataFrame(H_scenarios)\n",
    "    y = V_scenarios\n",
    "    \n",
    "    lasso = LassoCV(alphas=np.logspace(-4, 0, 20), cv=3, max_iter=5000)\n",
    "    lasso.fit(X, y)\n",
    "    \n",
    "    hedge_ratios = {instr: coef for instr, coef in zip(X.columns, lasso.coef_)}\n",
    "    return hedge_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e16697",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 9: Utility Functions for Backtest ===\n",
    "\n",
    "def get_window_dates(df_all, start_idx):\n",
    "    dates = np.sort(df_all['date'].unique())\n",
    "    if start_idx + TRADING_DAYS_PER_MONTH >= len(dates):\n",
    "        return None\n",
    "    return list(dates[start_idx:start_idx + TRADING_DAYS_PER_MONTH + 1])\n",
    "\n",
    "def choose_strike_for_m0(spot0, m0, strikes):\n",
    "    target = m0 * spot0\n",
    "    return float(strikes[np.abs(strikes - target).argmin()])\n",
    "\n",
    "def pick_one(day_df, strike, cp_flag):\n",
    "    sub = day_df[(day_df['strike_price'] == strike) & (day_df['cp_flag'] == cp_flag)].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    sub = sub.dropna(subset=['mid', 'delta', 'vega'])\n",
    "    sub = sub[sub['best_offer'] >= sub['best_bid']]\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    sub = sub.sort_values(by=['open_interest', 'volume'], ascending=[False, False])\n",
    "    return sub.iloc[0]\n",
    "\n",
    "def fetch_next_row(next_df, strike, cp_flag):\n",
    "    nxt = next_df[(next_df['strike_price'] == strike) & (next_df['cp_flag'] == cp_flag)]\n",
    "    if nxt.empty:\n",
    "        return None\n",
    "    nxt = nxt.dropna(subset=['mid'])\n",
    "    nxt = nxt.sort_values(by=['open_interest', 'volume'], ascending=[False, False])\n",
    "    return nxt.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ef015",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 10: Single Window Runner ===\n",
    "\n",
    "def run_volgan_window(df_all, gen, start_idx, m0, m_grid, tau_grid, \n",
    "                      condition_data, dates_data, device='cpu'):\n",
    "    \"\"\"Run one 21-day VolGAN hedging window.\"\"\"\n",
    "    days = get_window_dates(df_all, start_idx)\n",
    "    if days is None:\n",
    "        return None\n",
    "    \n",
    "    d0 = pd.Timestamp(days[0])\n",
    "    day0 = df_all[df_all['date'] == d0]\n",
    "    if day0.empty:\n",
    "        return None\n",
    "    \n",
    "    spot0 = float(day0['spot'].iloc[0])\n",
    "    strikes0 = np.sort(day0['strike_price'].unique())\n",
    "    K_straddle = choose_strike_for_m0(spot0, m0, strikes0)\n",
    "    \n",
    "    hedge_options = [\n",
    "        {'K': K_straddle, 'cp': 'C'},\n",
    "        {'K': K_straddle, 'cp': 'P'},\n",
    "        {'K': K_straddle * 0.9, 'cp': 'P'},\n",
    "        {'K': K_straddle * 1.1, 'cp': 'C'},\n",
    "    ]\n",
    "    \n",
    "    records = []\n",
    "    V0, Pi_t = None, None\n",
    "    phi_prev = {}\n",
    "    \n",
    "    for t in range(len(days) - 1):\n",
    "        d, d_next = pd.Timestamp(days[t]), pd.Timestamp(days[t + 1])\n",
    "        \n",
    "        day = df_all[df_all['date'] == d]\n",
    "        day_next = df_all[df_all['date'] == d_next]\n",
    "        \n",
    "        if day.empty or day_next.empty:\n",
    "            return None\n",
    "        \n",
    "        row_C = pick_one(day, K_straddle, 'C')\n",
    "        row_P = pick_one(day, K_straddle, 'P')\n",
    "        if row_C is None or row_P is None:\n",
    "            return None\n",
    "        \n",
    "        V_t = row_C['mid'] + row_P['mid']\n",
    "        spot_t = row_C['spot']\n",
    "        \n",
    "        if V0 is None:\n",
    "            V0 = V_t\n",
    "            Pi_t = V0\n",
    "        \n",
    "        # Get VolGAN condition\n",
    "        try:\n",
    "            date_idx = np.where(dates_data == d)[0][0]\n",
    "            condition_t = torch.tensor(condition_data[date_idx], dtype=torch.float, device=device)\n",
    "        except (IndexError, ValueError):\n",
    "            return None\n",
    "        \n",
    "        hedge_ratios = hedge_with_volgan(\n",
    "            gen, condition_t, spot_t, K_straddle, hedge_options,\n",
    "            m_grid, tau_grid, n_scenarios=500, device=device\n",
    "        )\n",
    "        \n",
    "        row_C_next = fetch_next_row(day_next, K_straddle, 'C')\n",
    "        row_P_next = fetch_next_row(day_next, K_straddle, 'P')\n",
    "        if row_C_next is None or row_P_next is None:\n",
    "            return None\n",
    "        \n",
    "        spot_t1 = row_C_next['spot']\n",
    "        V_t1 = row_C_next['mid'] + row_P_next['mid']\n",
    "        \n",
    "        pnl_total = 0\n",
    "        cost_total = 0\n",
    "        \n",
    "        phi_spot = hedge_ratios.get('spot', 0)\n",
    "        pnl_total += phi_spot * (spot_t1 - spot_t)\n",
    "        \n",
    "        for opt in hedge_options:\n",
    "            key = f\"{opt['cp']}_{opt['K']}\"\n",
    "            phi_opt = hedge_ratios.get(key, 0)\n",
    "            \n",
    "            opt_t = pick_one(day, opt['K'], opt['cp'])\n",
    "            opt_t1 = fetch_next_row(day_next, opt['K'], opt['cp'])\n",
    "            \n",
    "            if opt_t is not None and opt_t1 is not None:\n",
    "                pnl_total += phi_opt * (opt_t1['mid'] - opt_t['mid'])\n",
    "                phi_opt_prev = phi_prev.get(key, 0)\n",
    "                cost_total += abs(phi_opt - phi_opt_prev) * (opt_t['best_offer'] - opt_t['best_bid']) / 2.0\n",
    "        \n",
    "        Pi_t1 = Pi_t + pnl_total - cost_total\n",
    "        \n",
    "        records.append({\n",
    "            'date': d,\n",
    "            'V_t': V_t,\n",
    "            'Pi_t': Pi_t,\n",
    "            'Pi_t1': Pi_t1,\n",
    "            'pnl_hedged': pnl_total - cost_total,\n",
    "            'pnl_unhedged': V_t1 - V_t\n",
    "        })\n",
    "        \n",
    "        phi_prev = hedge_ratios.copy()\n",
    "        Pi_t = Pi_t1\n",
    "    \n",
    "    # Final valuation\n",
    "    last_day, final_day = pd.Timestamp(days[-2]), pd.Timestamp(days[-1])\n",
    "    day_last = df_all[df_all['date'] == last_day]\n",
    "    day_final = df_all[df_all['date'] == final_day]\n",
    "    \n",
    "    row_C_fin = fetch_next_row(day_final, K_straddle, 'C')\n",
    "    row_P_fin = fetch_next_row(day_final, K_straddle, 'P')\n",
    "    \n",
    "    if row_C_fin is None or row_P_fin is None:\n",
    "        return None\n",
    "    \n",
    "    V_T = row_C_fin['mid'] + row_P_fin['mid']\n",
    "    Pi_T = records[-1]['Pi_t1']\n",
    "    \n",
    "    summary = {\n",
    "        'start': days[0],\n",
    "        'end': days[-1],\n",
    "        'm0': m0,\n",
    "        'V0': V0,\n",
    "        'V_T': V_T,\n",
    "        'Pi_T': Pi_T,\n",
    "        'tracking_error': V_T - Pi_T,\n",
    "        'pnl_unhedged': V_T - V0,\n",
    "        'pnl_hedged': Pi_T - V0,\n",
    "        'strategy': 'VolGAN'\n",
    "    }\n",
    "    \n",
    "    return {'timeline': pd.DataFrame(records), 'summary': summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c11f35",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 11: Multi-Window Experiment ===\n",
    "\n",
    "def run_volgan_experiment(df_all, gen, m0_list, condition_data, dates_data,\n",
    "                          m_grid, tau_grid, max_windows=52, device='cpu'):\n",
    "    \"\"\"Run VolGAN across multiple windows.\"\"\"\n",
    "    dates = np.sort(df_all['date'].unique())\n",
    "    start_indices = list(range(0, len(dates) - TRADING_DAYS_PER_MONTH - 1, TRADING_DAYS_PER_MONTH))\n",
    "    start_indices = start_indices[:max_windows]\n",
    "    \n",
    "    results = []\n",
    "    for m0 in m0_list:\n",
    "        print(f\"\\n===== VolGAN: m0={m0} =====\")\n",
    "        for i, si in enumerate(start_indices, 1):\n",
    "            print(f\"Window {i}/{len(start_indices)}\")\n",
    "            out = run_volgan_window(\n",
    "                df_all, gen, si, m0, m_grid, tau_grid,\n",
    "                condition_data, dates_data, device\n",
    "            )\n",
    "            if out is not None:\n",
    "                results.append(out['summary'])\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e946cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 12: Prepare Backtest DataFrame ===\n",
    "\n",
    "# Filter options to 1-month maturity\n",
    "# Check if 'spot' already exists in df_options\n",
    "if 'spot' in df_options.columns:\n",
    "    print(\"'spot' column already exists in df_options\")\n",
    "    df = df_options.copy()\n",
    "else:\n",
    "    # If not, merge with df_underlying\n",
    "    df = df_options.merge(df_underlying, on='date', how='inner', suffixes=('', '_y'))\n",
    "    if 'spot_y' in df.columns:\n",
    "        df['spot'] = df['spot_y']\n",
    "        df = df.drop(columns=['spot_y'])\n",
    "\n",
    "# Ensure moneyness is computed\n",
    "if 'moneyness' not in df.columns:\n",
    "    df['moneyness'] = df['strike_price'] / df['spot']\n",
    "\n",
    "# Filter to ~1-month maturity (15-35 days)\n",
    "df_1m = df[(df['dte'] >= 15) & (df['dte'] <= 35)].copy()\n",
    "\n",
    "# Quality filters\n",
    "df_1m = df_1m.replace([np.inf, -np.inf], np.nan)\n",
    "df_1m = df_1m.dropna(subset=['mid', 'delta', 'vega', 'best_bid', 'best_offer', 'spot'])\n",
    "df_1m = df_1m[df_1m['best_offer'] >= df_1m['best_bid']]\n",
    "df_1m['spread'] = (df_1m['best_offer'] - df_1m['best_bid']).clip(lower=0)\n",
    "df_1m['rel_spread'] = df_1m['spread'] / df_1m['mid']\n",
    "\n",
    "# Liquidity filters\n",
    "liq = (\n",
    "    (df_1m['mid'] > 0.05) &\n",
    "    (df_1m['rel_spread'] <= 0.35) &\n",
    "    df_1m['delta'].between(-1.1, 1.1) &\n",
    "    (df_1m['vega'] > 0)\n",
    ")\n",
    "df_1m = df_1m[liq].copy()\n",
    "\n",
    "# Filter to backtest period\n",
    "start_backtest = pd.Timestamp(\"2021-01-01\")\n",
    "df_1m_bt = df_1m[df_1m['date'] >= start_backtest].copy()\n",
    "print(f\"Backtest  {len(df_1m_bt)} rows from {df_1m_bt['date'].min()} to {df_1m_bt['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 13: Filter Test Data for VolGAN ===\n",
    "\n",
    "# Define train/test split\n",
    "train_end = pd.Timestamp('2020-12-31')\n",
    "dates_pd = pd.to_datetime(dates_volgan_aligned)\n",
    "test_mask = dates_pd > train_end\n",
    "\n",
    "# Extract test data\n",
    "condition_test_raw = condition[test_mask]\n",
    "dates_test_raw = dates_pd[test_mask]\n",
    "\n",
    "# Filter to overlap with backtest dates\n",
    "dates_in_backtest = df_1m_bt['date'].unique()\n",
    "mask_overlap = np.isin(dates_test_raw, dates_in_backtest)\n",
    "\n",
    "condition_test_filtered = condition_test_raw[mask_overlap]\n",
    "dates_test_filtered = dates_test_raw[mask_overlap]\n",
    "\n",
    "print(f\"VolGAN test  {len(dates_test_filtered)} dates\")\n",
    "print(f\"Coverage: {dates_test_filtered.min()} to {dates_test_filtered.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8700e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === SECTION 13A: Initialize Models ===\n",
    "\n",
    "# Dimensions\n",
    "noise_dim = 20\n",
    "cond_dim = 3 + 80  # 3 (r_tm1, r_tm2, RV_tm1) + 80 (log IV surface)\n",
    "hidden_dim = 128\n",
    "output_dim = 1 + 80  # 1 (spot return) + 80 (log IV increment)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "gen = Generator(noise_dim=noise_dim, cond_dim=cond_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "disc = Discriminator(in_dim=cond_dim + output_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "gen.to(device)\n",
    "disc.to(device)\n",
    "\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in gen.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in disc.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84962c99",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b28419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 13B: Gradient Matching (FULLY CORRECTED) ===\n",
    "\n",
    "def gradient_matching(gen, disc, condition_train, true_train, m_grid, tau_grid, \n",
    "                     n_epochs=5, batch_size=64, lr_g=0.0002, lr_d=0.0002, device='cpu'):\n",
    "    \"\"\"Pre-training to find penalty weights.\"\"\"\n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    n_train = condition_train.shape[0]\n",
    "    n_batches = n_train // batch_size + 1\n",
    "    \n",
    "    lk, lt = len(m_grid), len(tau_grid)  # 10, 8\n",
    "    Ngrid = lk * lt  # 80\n",
    "    \n",
    "    # === Simplified smoothness penalties (L2 regularization on finite differences) ===\n",
    "    BCE_grads = []\n",
    "    m_smooth_grads = []\n",
    "    t_smooth_grads = []\n",
    "    \n",
    "    gen.train()\n",
    "    for epoch in tqdm(range(n_epochs), desc=\"Gradient matching\"):\n",
    "        perm = torch.randperm(n_train)\n",
    "        condition_train_perm = condition_train[perm]\n",
    "        true_train_perm = true_train[perm]\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            curr_batch_size = min(batch_size, n_train - i*batch_size)\n",
    "            condition = condition_train_perm[i*batch_size:i*batch_size + curr_batch_size]\n",
    "            real = true_train_perm[i*batch_size:i*batch_size + curr_batch_size]\n",
    "            surface_past = condition[:, 3:]\n",
    "            \n",
    "            # Update discriminator\n",
    "            disc_opt.zero_grad()\n",
    "            noise = torch.randn((curr_batch_size, gen.noise_dim), device=device, dtype=torch.float)\n",
    "            fake = gen(noise, condition)\n",
    "            \n",
    "            real_and_cond = torch.cat((condition, real), dim=-1)\n",
    "            fake_and_cond = torch.cat((condition, fake), dim=-1)\n",
    "            \n",
    "            disc_fake_pred = disc(fake_and_cond.detach())\n",
    "            disc_real_pred = disc(real_and_cond)\n",
    "            \n",
    "            disc_loss = (criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred)) +\n",
    "                        criterion(disc_real_pred, torch.ones_like(disc_real_pred))) / 2\n",
    "            disc_loss.backward()\n",
    "            disc_opt.step()\n",
    "            \n",
    "            # === Measure generator gradients ===\n",
    "            gen_opt.zero_grad()\n",
    "            noise = torch.randn((curr_batch_size, gen.noise_dim), device=device, dtype=torch.float)\n",
    "            fake = gen(noise, condition)\n",
    "            fake_surface = torch.exp(fake[:, 1:] + surface_past)  # Shape: (batch, 80)\n",
    "            \n",
    "            # Reshape to (batch, lk, lt) for easier differencing\n",
    "            fake_surface_grid = fake_surface.reshape(curr_batch_size, lk, lt)\n",
    "            \n",
    "            # Moneyness smoothness (differences along dim 1)\n",
    "            m_diffs = fake_surface_grid[:, 1:, :] - fake_surface_grid[:, :-1, :]  # (batch, lk-1, lt)\n",
    "            m_penalty = torch.mean(m_diffs ** 2)\n",
    "            \n",
    "            m_penalty.backward(retain_graph=True)\n",
    "            m_grad_norm = sum(p.grad.data.norm(2).item()**2 for p in gen.parameters() if p.grad is not None)**0.5\n",
    "            m_smooth_grads.append(m_grad_norm)\n",
    "            gen_opt.zero_grad()\n",
    "            \n",
    "            # Temporal smoothness (differences along dim 2)\n",
    "            t_diffs = fake_surface_grid[:, :, 1:] - fake_surface_grid[:, :, :-1]  # (batch, lk, lt-1)\n",
    "            t_penalty = torch.mean(t_diffs ** 2)\n",
    "            \n",
    "            t_penalty.backward(retain_graph=True)\n",
    "            t_grad_norm = sum(p.grad.data.norm(2).item()**2 for p in gen.parameters() if p.grad is not None)**0.5\n",
    "            t_smooth_grads.append(t_grad_norm)\n",
    "            gen_opt.zero_grad()\n",
    "            \n",
    "            # BCE gradient\n",
    "            fake_and_cond = torch.cat((condition, fake), dim=-1)\n",
    "            disc_fake_pred = disc(fake_and_cond)\n",
    "            gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "            gen_loss.backward()\n",
    "            bce_grad_norm = sum(p.grad.data.norm(2).item()**2 for p in gen.parameters() if p.grad is not None)**0.5\n",
    "            BCE_grads.append(bce_grad_norm)\n",
    "            gen_opt.step()\n",
    "    \n",
    "    # Compute penalty weights\n",
    "    alpha = np.mean(np.array(BCE_grads) / (np.array(m_smooth_grads) + 1e-8))\n",
    "    beta = np.mean(np.array(BCE_grads) / (np.array(t_smooth_grads) + 1e-8))\n",
    "    \n",
    "    print(f\"\\nGradient matching complete:\")\n",
    "    print(f\"  Alpha (moneyness penalty): {alpha:.4f}\")\n",
    "    print(f\"  Beta (time penalty): {beta:.4f}\")\n",
    "    \n",
    "    return gen, disc, gen_opt, disc_opt, alpha, beta\n",
    "\n",
    "# Run gradient matching\n",
    "gen, disc, gen_opt, disc_opt, alpha, beta = gradient_matching(\n",
    "    gen, disc, condition_train, true_train, m, tau,\n",
    "    n_epochs=3, batch_size=64, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f5d34",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a466e7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# === SECTION 13C: Full Training Loop (CORRECTED) ===\n",
    "\n",
    "def train_volgan(gen, disc, gen_opt, disc_opt, alpha, beta,\n",
    "                condition_train, true_train, m_grid, tau_grid,\n",
    "                n_epochs=100, batch_size=64, device='cpu'):\n",
    "    \"\"\"Main VolGAN training with adversarial + smoothness penalties.\"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    n_train = condition_train.shape[0]\n",
    "    n_batches = n_train // batch_size + 1\n",
    "    \n",
    "    lk, lt = len(m_grid), len(tau_grid)\n",
    "    \n",
    "    disc_losses = []\n",
    "    gen_losses = []\n",
    "    \n",
    "    gen.train()\n",
    "    disc.train()\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs), desc=\"Training VolGAN\"):\n",
    "        perm = torch.randperm(n_train)\n",
    "        condition_train_perm = condition_train[perm]\n",
    "        true_train_perm = true_train[perm]\n",
    "        \n",
    "        epoch_disc_loss = 0\n",
    "        epoch_gen_loss = 0\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            curr_batch_size = min(batch_size, n_train - i*batch_size)\n",
    "            condition = condition_train_perm[i*batch_size:i*batch_size + curr_batch_size]\n",
    "            real = true_train_perm[i*batch_size:i*batch_size + curr_batch_size]\n",
    "            surface_past = condition[:, 3:]\n",
    "            \n",
    "            # ===== Update Discriminator =====\n",
    "            disc_opt.zero_grad()\n",
    "            noise = torch.randn((curr_batch_size, gen.noise_dim), device=device, dtype=torch.float)\n",
    "            fake = gen(noise, condition)\n",
    "            \n",
    "            real_and_cond = torch.cat((condition, real), dim=-1)\n",
    "            fake_and_cond = torch.cat((condition, fake), dim=-1)\n",
    "            \n",
    "            disc_fake_pred = disc(fake_and_cond.detach())\n",
    "            disc_real_pred = disc(real_and_cond)\n",
    "            \n",
    "            disc_loss = (criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred)) +\n",
    "                        criterion(disc_real_pred, torch.ones_like(disc_real_pred))) / 2\n",
    "            disc_loss.backward()\n",
    "            disc_opt.step()\n",
    "            \n",
    "            epoch_disc_loss += disc_loss.item()\n",
    "            \n",
    "            # ===== Update Generator =====\n",
    "            gen_opt.zero_grad()\n",
    "            noise = torch.randn((curr_batch_size, gen.noise_dim), device=device, dtype=torch.float)\n",
    "            fake = gen(noise, condition)\n",
    "            fake_and_cond = torch.cat((condition, fake), dim=-1)\n",
    "            \n",
    "            disc_fake_pred = disc(fake_and_cond)\n",
    "            gen_bce_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "            \n",
    "            # Smoothness penalties (simplified tensor version)\n",
    "            fake_surface = torch.exp(fake[:, 1:] + surface_past)\n",
    "            fake_surface_grid = fake_surface.reshape(curr_batch_size, lk, lt)\n",
    "            \n",
    "            # Moneyness smoothness\n",
    "            m_diffs = fake_surface_grid[:, 1:, :] - fake_surface_grid[:, :-1, :]\n",
    "            m_penalty = torch.mean(m_diffs ** 2)\n",
    "            \n",
    "            # Temporal smoothness\n",
    "            t_diffs = fake_surface_grid[:, :, 1:] - fake_surface_grid[:, :, :-1]\n",
    "            t_penalty = torch.mean(t_diffs ** 2)\n",
    "            \n",
    "            # Total generator loss\n",
    "            gen_loss = gen_bce_loss + alpha * m_penalty + beta * t_penalty\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            \n",
    "            epoch_gen_loss += gen_loss.item()\n",
    "        \n",
    "        # Log every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            avg_disc = epoch_disc_loss / n_batches\n",
    "            avg_gen = epoch_gen_loss / n_batches\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} | D_loss: {avg_disc:.4f} | G_loss: {avg_gen:.4f}\")\n",
    "        \n",
    "        disc_losses.append(epoch_disc_loss / n_batches)\n",
    "        gen_losses.append(epoch_gen_loss / n_batches)\n",
    "    \n",
    "    return gen, disc, disc_losses, gen_losses\n",
    "\n",
    "# Train VolGAN\n",
    "gen, disc, disc_losses, gen_losses = train_volgan(\n",
    "    gen, disc, gen_opt, disc_opt, alpha, beta,\n",
    "    condition_train, true_train, m, tau,\n",
    "    n_epochs=50, batch_size=64, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8a2d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# === SECTION 13D: Save Trained Model ===\n",
    "\n",
    "torch.save(gen.state_dict(), 'volgan_trained.pt')\n",
    "torch.save(disc.state_dict(), 'volgan_discriminator.pt')\n",
    "torch.save({\n",
    "    'alpha': alpha,\n",
    "    'beta': beta,\n",
    "    'gen_losses': gen_losses,\n",
    "    'disc_losses': disc_losses\n",
    "}, 'volgan_training_stats.pt')\n",
    "\n",
    "print(\"\\nModel saved successfully!\")\n",
    "print(\"  - Generator: volgan_trained.pt\")\n",
    "print(\"  - Discriminator: volgan_discriminator.pt\")\n",
    "print(\"  - Training stats: volgan_training_stats.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacffea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 13E: Plot Training Loss ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(disc_losses, label='Discriminator Loss')\n",
    "ax1.set_title('Discriminator Loss Over Training')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(gen_losses, label='Generator Loss', color='orange')\n",
    "ax2.set_title('Generator Loss Over Training')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6114530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 14: Load Trained VolGAN Model ===\n",
    "\n",
    "gen = Generator(noise_dim=20, cond_dim=3+80, hidden_dim=128, output_dim=1+80)\n",
    "gen.load_state_dict(torch.load('volgan_trained.pt', map_location=device))\n",
    "gen.to(device)\n",
    "gen.eval()\n",
    "print(\"VolGAN model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15562e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 15: Run VolGAN Backtest ===\n",
    "\n",
    "m0_list = [0.75, 0.8, 0.9, 1.1, 1.2, 1.25]\n",
    "\n",
    "results_volgan = run_volgan_experiment(\n",
    "    df_1m_bt, gen, m0_list,\n",
    "    condition_test_filtered,\n",
    "    dates_test_filtered,\n",
    "    m_grid, tau, max_windows=52, device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nVolGAN results: {results_volgan.shape}\")\n",
    "results_volgan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f19bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 16: Summary Statistics ===\n",
    "\n",
    "summary_volgan = results_volgan.groupby('m0')['tracking_error'].agg([\n",
    "    'mean', 'median', 'std',\n",
    "    ('VaR_5%', lambda x: x.quantile(0.05))\n",
    "]).round(3)\n",
    "\n",
    "print(\"\\nVolGAN Tracking Error Summary:\")\n",
    "print(summary_volgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36c85a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f288f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 17: VolGAN Results — Box Plot by Moneyness ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='m0', y='tracking_error', data=results_volgan, palette='Set2')\n",
    "plt.title('VolGAN Tracking Error Distribution by Moneyness\\n(21-Day Windows)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Moneyness m₀ = K/S₀', fontsize=12)\n",
    "plt.ylabel('Tracking Error Z_T = V_T − Π_T (index points)', fontsize=12)\n",
    "plt.axhline(y=0, color='red', linestyle='--', alpha=0.6, linewidth=2, label='Perfect Hedge')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 18: Summary Statistics Table ===\n",
    "\n",
    "summary_volgan = results_volgan.groupby('m0')['tracking_error'].agg([\n",
    "    ('Count', 'count'),\n",
    "    ('Mean', 'mean'),\n",
    "    ('Median', 'median'),\n",
    "    ('Std Dev', 'std'),\n",
    "    ('Min', 'min'),\n",
    "    ('Max', 'max'),\n",
    "    ('VaR 5%', lambda x: x.quantile(0.05)),\n",
    "    ('VaR 1%', lambda x: x.quantile(0.01))\n",
    "]).round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VolGAN Tracking Error Summary by Moneyness\")\n",
    "print(\"=\"*80)\n",
    "print(summary_volgan)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2184bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 19: Tracking Error Time Series ===\n",
    "\n",
    "res = results_volgan.sort_values('end').reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(res['end'], res['tracking_error'], marker='o', color='steelblue', \n",
    "         linewidth=2, markersize=4)\n",
    "plt.axhline(0, color='red', linestyle='--', alpha=0.6, linewidth=2, label='Zero Error')\n",
    "plt.title('VolGAN Tracking Error per Window (Chronological)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Window End Date', fontsize=12)\n",
    "plt.ylabel('Tracking Error Z_T (index points)', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION 20: Cumulative PnL (Hedged vs Unhedged) ===\n",
    "\n",
    "res = results_volgan.sort_values('end').reset_index(drop=True)\n",
    "res['cum_unhedged'] = res['pnl_unhedged'].cumsum()\n",
    "res['cum_hedged'] = res['pnl_hedged'].cumsum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(res['end'], res['cum_unhedged'], label='Cumulative Unhedged', \n",
    "         color='cornflowerblue', linewidth=2)\n",
    "plt.plot(res['end'], res['cum_hedged'], label='Cumulative VolGAN-Hedged', \n",
    "         color='darkorange', linewidth=2)\n",
    "plt.axhline(0, color='gray', linestyle='--', alpha=0.6)\n",
    "plt.title('Cumulative PnL Over Non-Overlapping Windows (VolGAN)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Window End Date', fontsize=12)\n",
    "plt.ylabel('Cumulative PnL (index points)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dbfdc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# === SECTION 21: Per-Window PnL (Hedged vs Unhedged) ===\n",
    "\n",
    "res = results_volgan.sort_values('end').reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(res['end'], res['pnl_unhedged'], label='Unhedged PnL', \n",
    "         color='steelblue', linewidth=2)\n",
    "plt.plot(res['end'], res['pnl_hedged'], label='VolGAN-Hedged PnL', \n",
    "         color='orangered', linewidth=2)\n",
    "plt.axhline(0, linestyle='--', color='black', alpha=0.6)\n",
    "plt.title('Per-Window PnL: VolGAN Hedging vs Unhedged', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Window End Date', fontsize=12)\n",
    "plt.ylabel('PnL (index points)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
