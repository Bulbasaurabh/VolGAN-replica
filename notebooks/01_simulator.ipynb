{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1634b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pip install -r requirements.txt (from the root directory) to install the dependencies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec08615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7966273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an optionid to simulate\n",
    "selected_option_id = 10010807"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a858b",
   "metadata": {},
   "source": [
    "### 1. Load Dataset\n",
    "We will load it with Polars first (suitable for large files) and convert it to a Pandas dataframe when we have filtered the selected optionid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b10e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------\n",
    "\n",
    "# Lazy load with polars (memory efficient for large files)\n",
    "options_df = pl.scan_parquet(\"../data/options_dataset.parquet\")\n",
    "\n",
    "# Filter for our selected optionid\n",
    "options_df = options_df.filter(pl.col(\"optionid\") == selected_option_id).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af9553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert polars df to pandas df for easier handling\n",
    "options_df = options_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b6eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_df[\"date\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1bceb5",
   "metadata": {},
   "source": [
    "### 2. Preprocess Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. Preprocess data\n",
    "# -------------------------\n",
    "\n",
    "options_df['date'] = pd.to_datetime(options_df['date'], errors='coerce')\n",
    "\n",
    "# Add mid-price column\n",
    "options_df[\"mid_price\"] = (options_df[\"best_bid\"] + options_df[\"best_offer\"]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66265c",
   "metadata": {},
   "source": [
    "### 3. Load underlying SPX prices for the option days\n",
    "We will download the SPX prices from yfinance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62546c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3. Download and load underlying SPX prices\n",
    "# -------------------------\n",
    "\n",
    "ticker = \"^SPX\"\n",
    "start_date = options_df[\"date\"].min()\n",
    "end_date = options_df[\"date\"].max()\n",
    "\n",
    "spx_df = yf.download(ticker, start=start_date, end=end_date)\n",
    "spx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7280c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_df = spx_df[['Close']].reset_index().rename(columns={'Date': 'date', 'Close': 'spx_close'})\n",
    "spx_df.columns = [col[0] if isinstance(col, tuple) else col for col in spx_df.columns]\n",
    "spx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_df = options_df.merge(spx_df, on='date', how='left')\n",
    "\n",
    "options_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a394d39b",
   "metadata": {},
   "source": [
    "### 4. Delta-hedge Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. Delta-hedge simulation (daily)\n",
    "# -------------------------\n",
    "position_option = 1  # long 1 contract\n",
    "contract_size = 100  # each option contract controls 100 shares\n",
    "cash = 0\n",
    "underlying_pos = 0\n",
    "transaction_cost_pct = 0.001  # 0.1% per trade\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in options_df.iterrows():\n",
    "    date = row['date']\n",
    "    spx_price = row['spx_close']\n",
    "    delta = row['delta']\n",
    "    mid_price = row['mid_price']\n",
    "\n",
    "    # Target underlying position to delta-hedge\n",
    "    target_underlying = - position_option * delta * contract_size\n",
    "    trade = target_underlying - underlying_pos\n",
    "    trade_notional = abs(trade) * spx_price\n",
    "    tx_cost = trade_notional * transaction_cost_pct\n",
    "\n",
    "    # Update cash and underlying position\n",
    "    cash -= trade * spx_price + tx_cost\n",
    "    underlying_pos = target_underlying\n",
    "\n",
    "    # Mark-to-market\n",
    "    option_value = mid_price * position_option * contract_size\n",
    "    underlying_value = underlying_pos * spx_price\n",
    "    total_value = cash + option_value + underlying_value\n",
    "\n",
    "    results.append({\n",
    "        'date': date,\n",
    "        'option_value': option_value,\n",
    "        'underlying_value': underlying_value,\n",
    "        'cash': cash,\n",
    "        'total_value': total_value,\n",
    "        'tx_cost': tx_cost\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5480d",
   "metadata": {},
   "source": [
    "### 5. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 5. Plot results\n",
    "# -------------------------\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(results_df['date'], results_df['total_value'], label='Total Portfolio Value')\n",
    "plt.plot(results_df['date'], results_df['option_value'], label='Option Value', linestyle='--')\n",
    "plt.plot(results_df['date'], results_df['underlying_value'], label='Underlying Hedge Value', linestyle=':')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('USD')\n",
    "plt.title('Delta-Hedge Backtest')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912275c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Black Scholes Formula\n",
    "def simulate_gbm(S0, mu, sigma, T, n_steps, n_paths, seed=None):\n",
    "    \"\"\"\n",
    "    Simulate GBM (Black-Scholes) asset paths with exact discretization.\n",
    "\n",
    "    Returns:\n",
    "      times: array shape (n_steps+1,)\n",
    "      S: array shape (n_paths, n_steps+1)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dt = T / n_steps\n",
    "    times = np.linspace(0.0, T, n_steps + 1)\n",
    "    # generate standard normals: shape (n_paths, n_steps)\n",
    "    Z = rng.standard_normal(size=(n_paths, n_steps))\n",
    "    # increments of log S\n",
    "    drift = (mu - 0.5 * sigma**2) * dt\n",
    "    diffusion = sigma * np.sqrt(dt) * Z\n",
    "    log_increments = drift + diffusion\n",
    "    logS = np.concatenate([np.log(np.full((n_paths, 1), S0)), np.cumsum(log_increments, axis=1) + np.log(S0)], axis=1)\n",
    "    S = np.exp(logS)\n",
    "    return times, S\n",
    "\n",
    "times, S_paths = simulate_gbm(S0=100, mu=0.0, sigma=0.2, T=1.0, n_steps=252, n_paths=1000, seed=42)\n",
    "# S_paths.shape -> (1000, 253)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab602637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_heston_full_truncation(S0, v0, mu, kappa, theta, xi, rho,\n",
    "                                    T, n_steps, n_paths, seed=None, reflect_floor=0.0):\n",
    "    \"\"\"\n",
    "    Simulate Heston model using Euler-Maruyama with full truncation for variance.\n",
    "    - S0: initial spot\n",
    "    - v0: initial variance (not vol)\n",
    "    - mu: drift of S (use r or 0 for risk-neutral/driftless)\n",
    "    - kappa, theta: mean-reversion speed and level of variance\n",
    "    - xi: volatility of variance (vol-of-vol)\n",
    "    - rho: correlation between asset and variance Brownian motions (in [-1,1])\n",
    "    - T, n_steps, n_paths: time horizon, steps, number of simulated paths\n",
    "    - seed: RNG seed\n",
    "    - reflect_floor: minimum allowed variance (small positive, e.g., 1e-8). If 0, we clip at 0.\n",
    "    Returns:\n",
    "      times: (n_steps+1,)\n",
    "      S: (n_paths, n_steps+1)\n",
    "      v: (n_paths, n_steps+1)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dt = T / n_steps\n",
    "    sqrt_dt = np.sqrt(dt)\n",
    "    times = np.linspace(0.0, T, n_steps + 1)\n",
    "\n",
    "    S = np.zeros((n_paths, n_steps + 1))\n",
    "    v = np.zeros((n_paths, n_steps + 1))\n",
    "    S[:, 0] = S0\n",
    "    v[:, 0] = v0\n",
    "\n",
    "    # Generate correlated normals: shape (n_paths, n_steps)\n",
    "    # Use independent normals then mix\n",
    "    Z1 = rng.standard_normal((n_paths, n_steps))\n",
    "    Z2 = rng.standard_normal((n_paths, n_steps))\n",
    "    W1 = Z1\n",
    "    W2 = rho * Z1 + np.sqrt(1 - rho**2) * Z2  # correlated normals\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        vt = np.maximum(v[:, t], 0.0)  # ensure non-neg before sqrt\n",
    "        # variance (v) update - full truncation: premultiply noise by sqrt(max(v_t,0))\n",
    "        dv = kappa * (theta - vt) * dt + xi * np.sqrt(np.maximum(vt, 0.0)) * sqrt_dt * W2[:, t]\n",
    "        v_new = v[:, t] + dv\n",
    "        # full truncation: ensure drift uses max(v_t,0) and then clamp v_new to >= 0 (or reflect_floor)\n",
    "        v_new = np.maximum(v_new, reflect_floor)\n",
    "\n",
    "        # asset price update using sqrt(max(v_t,0)) for diffusion\n",
    "        diffusion_term = np.sqrt(np.maximum(vt, 0.0)) * sqrt_dt * W1[:, t]\n",
    "        S_new = S[:, t] * np.exp((mu - 0.5 * vt) * dt + diffusion_term)  # log-Euler style\n",
    "\n",
    "        v[:, t + 1] = v_new\n",
    "        S[:, t + 1] = S_new\n",
    "\n",
    "    return times, S, v\n",
    "times, S_heston, v_heston = simulate_heston_full_truncation(\n",
    "    S0=100, v0=0.04, mu=0.0, kappa=2.0, theta=0.04, xi=0.5, rho=-0.7,\n",
    "    T=1.0, n_steps=252, n_paths=1000, seed=1234\n",
    ")\n",
    "# S_heston.shape -> (1000, 253)\n",
    "# v_heston.shape -> (1000, 253)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0696a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot a few sample paths\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(20):\n",
    "    plt.plot(times, S_paths[i, :], alpha=0.7)   # GBM\n",
    "plt.title(\"Example GBM paths (20 samples)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"S\")\n",
    "plt.show()\n",
    "\n",
    "# Heston asset + sample variance\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(6):\n",
    "    plt.plot(times, S_heston[i, :], alpha=0.8)\n",
    "plt.title(\"Heston asset sample paths\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"S\"); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(6):\n",
    "    plt.plot(times, v_heston[i, :], alpha=0.8)\n",
    "plt.title(\"Heston variance sample paths\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"v (variance)\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "# === 1. Load your Parquet data (already done) ===\n",
    "options_df = pd.read_parquet(\"../data/options_dataset.parquet\")\n",
    "\n",
    "# === 2. Basic cleaning ===\n",
    "options_df = options_df.dropna(subset=[\"impl_volatility\", \"strike_price\"])\n",
    "options_df[\"date\"] = pd.to_datetime(options_df[\"date\"])\n",
    "options_df[\"exdate\"] = pd.to_datetime(options_df[\"exdate\"])\n",
    "\n",
    "# === 3. Compute time-to-maturity (τ in years) and moneyness ===\n",
    "# Need underlying price per date — here we approximate using ATM strike midpoint\n",
    "spot_estimate = (\n",
    "    options_df.groupby(\"date\")[\"strike_price\"]\n",
    "    .median()\n",
    "    .rename(\"spot\")\n",
    ")\n",
    "options_df = options_df.merge(spot_estimate, on=\"date\")\n",
    "options_df[\"moneyness\"] = options_df[\"strike_price\"] / options_df[\"spot\"]\n",
    "options_df[\"tau\"] = (options_df[\"exdate\"] - options_df[\"date\"]).dt.days / 365.0\n",
    "\n",
    "# === 4. Define fixed (m, τ) grid ===\n",
    "m_grid = np.linspace(0.8, 1.2, 9)     # 0.8 → 1.2\n",
    "tau_grid = np.linspace(0.05, 0.5, 10) # 0.05y (~18d) → 0.5y (~6mo)\n",
    "M, T = np.meshgrid(m_grid, tau_grid)\n",
    "grid_points = np.column_stack([M.ravel(), T.ravel()])\n",
    "\n",
    "# === 5. Interpolate IV surface for each trading date ===\n",
    "surfaces = []\n",
    "dates = []\n",
    "for date, group in options_df.groupby(\"date\"):\n",
    "    pts = group[[\"moneyness\", \"tau\"]].to_numpy()\n",
    "    vols = group[\"impl_volatility\"].to_numpy()\n",
    "\n",
    "    # Skip days with too few points\n",
    "    if len(vols) < 20:\n",
    "        continue\n",
    "\n",
    "    # Interpolate implied vols to fixed grid\n",
    "    grid_vols = griddata(pts, vols, grid_points, method=\"linear\", fill_value=np.nan)\n",
    "    # Fill remaining NaNs with nearest\n",
    "    if np.isnan(grid_vols).any():\n",
    "        grid_vols = griddata(pts, vols, grid_points, method=\"nearest\", fill_value=np.nan)\n",
    "\n",
    "    surfaces.append(grid_vols)\n",
    "    dates.append(date)\n",
    "\n",
    "# Convert to DataFrame (each row = flattened surface)\n",
    "surf_df = pd.DataFrame(surfaces, index=dates)\n",
    "surf_df.index.name = \"date\"\n",
    "\n",
    "# === 6. Save to CSV (VolGAN expects CSV of flattened surfaces) ===\n",
    "output_dir = \"../data\"\n",
    "surf_df.to_csv(f\"{output_dir}/surfacesTransform.csv\", index=True)\n",
    "print(f\"Saved surfacesTransform.csv with shape {surf_df.shape}\")\n",
    "\n",
    "# === 7. Optionally, create a simple SPX.csv (for datapath) ===\n",
    "spx_df = pd.DataFrame({\n",
    "    \"Date\": dates,\n",
    "    \"SPX\": [spot_estimate.loc[d] for d in dates]\n",
    "})\n",
    "spx_df[\"log_return\"] = np.log(spx_df[\"SPX\"] / spx_df[\"SPX\"].shift(1))\n",
    "spx_df.to_csv(f\"{output_dir}/SPX.csv\", index=False)\n",
    "print(f\"Saved SPX.csv with shape {spx_df.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
